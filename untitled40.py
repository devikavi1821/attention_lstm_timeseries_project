# -*- coding: utf-8 -*-
"""Untitled40.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xWcOzjBbXUYlb18jtCsgN-GpYe_NNwG1
"""

# -------------------------------
# 1. Libraries
# -------------------------------
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Layer
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# -------------------------------
# 2. Generate Synthetic Multivariate Time Series Dataset
# -------------------------------
np.random.seed(42)
time_steps = 200
features = 3
n_samples = 1000

def generate_synthetic_series(n_samples, time_steps, features):
    t = np.arange(time_steps)
    data = []
    for i in range(n_samples):
        series = []
        for f in range(features):
            trend = 0.05 * t
            seasonality = np.sin(2 * np.pi * t / 20 + np.random.rand())
            noise = np.random.normal(0, 0.2, size=time_steps)
            series.append(trend + seasonality + noise)
        series = np.array(series).T  # shape (time_steps, features)
        data.append(series)
    return np.array(data)  # shape (n_samples, time_steps, features)

data = generate_synthetic_series(n_samples, time_steps, features)

# Target: predict next value of first feature
X = data[:, :-1, :]
y = data[:, 1:, 0]

# -------------------------------
# 3. Train/Test Split
# -------------------------------
split = int(0.8 * n_samples)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# -------------------------------
# 4. Feature Scaling
# -------------------------------
scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

n_train_samples, n_timesteps, n_features = X_train.shape
X_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, n_features)).reshape(n_train_samples, n_timesteps, n_features)
y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1)).reshape(n_train_samples, n_timesteps)

n_test_samples = X_test.shape[0]
X_test_scaled = scaler_X.transform(X_test.reshape(-1, n_features)).reshape(n_test_samples, n_timesteps, n_features)
y_test_scaled = scaler_y.transform(y_test.reshape(-1,1)).reshape(n_test_samples, n_timesteps)

# -------------------------------
# 5. Custom Attention Layer
# -------------------------------
class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], input_shape[-1]),
                                 initializer="random_normal", trainable=True)
        self.b = self.add_weight(name="att_bias", shape=(input_shape[-1],), initializer="zeros", trainable=True)
        super(Attention, self).build(input_shape)

    def call(self, x):
        score = tf.nn.tanh(tf.tensordot(x, self.W, axes=1) + self.b)
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = attention_weights * x
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights

# -------------------------------
# 6. Attention-Augmented LSTM Model
# -------------------------------
input_layer = Input(shape=(n_timesteps, n_features))
lstm_out = LSTM(64, return_sequences=True)(input_layer)
context_vector, att_weights = Attention()(lstm_out)
output_layer = Dense(n_timesteps)(context_vector)
model = Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer=Adam(0.001), loss='mse')
model.summary()

# -------------------------------
# 7. Train Model
# -------------------------------
history = model.fit(X_train_scaled, y_train_scaled, validation_split=0.1, epochs=30, batch_size=32)

# -------------------------------
# 8. Predictions
# -------------------------------
y_pred_scaled = model.predict(X_test_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled)

# -------------------------------
# 9. Evaluation
# -------------------------------
mae = mean_absolute_error(y_test.flatten(), y_pred.flatten())
rmse = np.sqrt(mean_squared_error(y_test.flatten(), y_pred.flatten()))
print(f"Test MAE: {mae:.4f}, Test RMSE: {rmse:.4f}")

# -------------------------------
# 10. Attention Weights Visualization
# -------------------------------
context_model = Model(inputs=input_layer, outputs=att_weights)
attention_output = context_model.predict(X_test_scaled[:5])  # first 5 samples

for i, att in enumerate(attention_output):
    plt.figure(figsize=(10,3))
    plt.imshow(att.T, cmap='hot', interpolation='nearest', aspect='auto')
    plt.title(f"Attention Heatmap for Sample {i+1}")
    plt.xlabel("Time Steps")
    plt.ylabel("Features")
    plt.colorbar()
    plt.show()

# -------------------------------
# 11. Save Model
# -------------------------------
model.save("attention_lstm_timeseries.h5")